<html xmlns="http://www.w3.org/1999/xhtml"><head>


<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"><title>CS651 Computer Vision: Assignment 2</title>



<td align="right" valign="middle"><font size="+3">Jack Lanchantin</font></td>
<br>
email: <a href="mailto:jjl5sw@virginia.edu">jjl5sw@cs.virginia.edu</a><br>

<hr>
<font color = "#7C27CB" size="+3">Computer Vision: Assignment 2</font><br>
<font color = "#7C27CB" size="+2">Face Detection Using Gaussian Distributions and Logistic Regression</font>

     

<p>This assignment's goal was to demonstrate the proper extraction of faces in images. I did this by implementing the following two different classifiers:<br><br>


<strong>1. Gaussian Distribution </strong><br>
The Gaussian distribution classifier seeks to produce a multivariate Gaussian for the test image using the train images for the mean and standard deviation. It computes both the distribution using the mean and std dev for the face images, and the distribution using the mean and std dev for the non faces. It then compares the probabilities of the two and chooses the one with greater probability. Since I was not getting good results using the regular multivariate gaussian distribution, I also enduced a prior probability on the output, assuming that the prior probability of seeing a face for all patches is about 0.001%.<br>


<br><strong>2. Logistic Regression</strong><br> 
Logistic regression seeks to find a linear separator between all of the faces and all of the non faces. This method is 
trained to find w, the vector which separates the 2 classes. <br>


<br><strong>Analysis</strong><br> 
It seems as though the logistic regression method is a better face detector. This is probably due to the fact that it is a discriminative classification model, which tends to perform better on tasks such as face detection. It is also very hard to model what a non-face looks like because there are so many possibilities. Since the gaussian classifier uses the means and standard deviations of the training images directly, it does not produce great results. The logistic regression classifier on the other hand, learns the separator itself, which is why it performs slightly better. In a way, the logistic regression classifier "builds" a representation of a face in w, whereas the gaussian classifier simply compares faces vs non faces using prior data <br><br>

One of the main reasons my models fail to produce close to perfect results in the gaussian pyramid representaion is that sometimes the detector detects the full face at one level of the gaussian pyramid, but it is not quite as strong as another level, so it gets removed during the non maximum suppression. If I could improve on the strength of the actual faces, then it would perform better. One way to do this would be to use more images in the training data. As you can see in the images, there are many places where the image detector falsely triggers. This happens in many cases where there is a strong change from dark to light to dark again (as seen in the mean face image).<br><br>

The reason that neither of these methods tend to give very good results overall is due to 3 major reasons: <br>
<strong>1.</strong> We are only using 144 dimensional representations of the data. This is a very small number and it means that we are losing a lot of the information contained in the image since we compress each image larger than 12x12 to this size. <br>
<strong>2.</strong> We are simply using the raw greyscale pixel values as features. In good face detectors, some feature representaion of the patches would be used instead. <br>
<strong>3.</strong> Lastly, we are only using 200 training images. This is extremely small and will never produce very reliable results. In order for a face detector to work well, we would need thousands or millions of images.<br>


<hr>
<strong>Readme:</strong><br> 
My program is run using python2 by running python ./FaceDetector.py <br>
The parameter and files to specify are declared within the file <br>

     
     <hr>
     <hr>

     <font color="#336699" size="+5">Training Data</font>
     <br>
     My program was trained on 100 faces from the MIT-CMU face datasets, and 100 non faces taken from randomly sampled
     patches from images in the MIT-CMU face dataset that did not contain faces. The training data is shown in the following image, where the left half is the positive faces, and the right half is the negative non-faces.

      <br>

      <br>
      <strong>Training Data:</strong>
      <br>
      <img src="./collage.png" />



     <hr>
     <hr>

     <font color="#336699" size="+5">Gaussian Distribution</font>
     <br>
     <br>
      <strong>Parameters:</strong><br>
      &#964;<sub>pos</sub> = 8.2<br>
      &#964;<sub>neg</sub> = 4.2<br>
      prior face probability = 0.001 <br>
      non maximum suppression window range: 20 pixels <br>
     The following 2 plots show the s values for Sigular Value Decomposition for both face images and non-face images
      <br>
      <br>
      <strong>face S Value:</strong>
      <br>
      <img src="./FaceSValue.png" height="400" width="400" />
       <strong></strong><br>
      <strong>non face S Value:</strong>
      <br>
      <img src="./nonFaceSValue.png" height="400" width="400" />
      <strong></strong><br>

      I chose different tau values for each of the gaussian distributions since they produce different s values. I chose 8.2 for faces and 4.2 for non faces.
      <br>
      <br>

      The following plots show the means of the face and non face images.
      <br>
      <hr>
      <br>
      <strong>Face Mean:</strong>
      <br>
      <img src="./meanPosFace.png" height="400" width="400"/>
       <strong></strong><br>
      <strong>Non Face Mean:</strong>
      <br>
      <img src="./meanNegFace.png" height="400" width="400" />
      <strong></strong><br>

      The following are sample output images from my Gaussian Distribution classifier:
       <strong></strong><br>
       <img src="./testResults/GAUSSIAN_nens0.png"/>
      <strong></strong><br>
       <img src="./testResults/GAUSSIAN_next0.png"/>
      <strong></strong><br>
      <img src="./testResults/GAUSSIAN_ew-friends0.png"/>
      <strong></strong><br>
      <img src="./testResults/GAUSSIAN_aerosmith-double0.png"/>
      <strong></strong><br>
       <img src="./testResults/GAUSSIAN_frisbee0.png"/>
      <strong></strong><br>



     <hr>
     <hr>

     <font color="#336699" size="+5">Logistic Regression</font>
     <br>
     <strong>Parameters:</strong><br>
      number of training iterations: 10,000<br>
      learning rate = 0.03<br>
      non maximum suppression window range: 20 pixels <br>

      &#964;<sub>neg</sub> = 4.2<br>

     The following picture shows the accuracy of the training data with respect to the number of iterations.
     <br>
      <img src="./logisticIterations.png"/>
      <strong></strong><br>

      The following are sample output images from my logistic regression classifier:
      <strong></strong><br>
       <img src="./testResults/LOGISTIC_nens0.png"/>
      <strong></strong><br>
       <img src="./testResults/LOGISTIC_next0.png"/>
      <strong></strong><br>
      <img src="./testResults/LOGISTIC_ew-friends0.png"/>
      <strong></strong><br>
      <img src="./testResults/LOGISTIC_aerosmith-double0.png"/>
      <strong></strong><br>
       <img src="./testResults/LOGISTIC_frisbee0.png"/>
      <strong></strong><br>



      <br>
      <br>

      <align="center">
      <footer class="row">
        <div class="large-12 columns">
          <hr/>
          <div class="row">
            <div class="large-6 columns">
              <p>* HTML Templates taken from: http://foundation.zurb.com/templates.html on 2/2/15, and Michael Holroyd's Report</p>
              <p>** I submitted this assigment 2 days late </p>
            </div>
            <div class="large-6 columns">
              <ul class="inline-list right">
              </ul>
            </div>
          </div>
        </div> 
      </footer>

</div>
</body></html>
    